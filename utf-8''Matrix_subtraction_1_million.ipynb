{
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "pc3TfwQ7PZhb",
        "outputId": "d658a96d-9861-4a5d-8083-18556c26ff0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport os\nfrom sympy import *\nimport random\nimport re\nfrom operator import add, abs, sub, mul, mod\ninit_printing(use_unicode=True)\nimport time\nimport numpy as np\n\ntry:\n  %tensorflow_version 2.x # enable TF 2.x in Colab\nexcept Exception:\n  pass\nfrom collections import namedtuple\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom sklearn.model_selection import train_test_split\nimport pickle\nfrom nltk.translate.bleu_score import corpus_bleu",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JnCgKUrrPjQs",
        "outputId": "f6c0ca33-11e1-45c0-a990-c9d089ceca16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "tf.__version__",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "'1.1.0'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "id": "R2JSi2MP-YpW",
        "colab_type": "code",
        "outputId": "b55fbab0-2b35-468b-c0eb-f3916f2081b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "questions = []\nanswers = []\n# samples = 10\nsamples = 1000000\n\n\nfor i in range(samples):\n    a = random.randint(1,4)\n    b = random.randint(2,3)\n    l1 = np.random.randint(1,100,(a,b))\n    l2 = np.random.randint(1,100,(a,b))\n\n    M = []\n    N = []\n    for i in l1:\n        temp=[]\n        for j in i:\n            temp.append(j)\n        M.append(temp)\n\n    for i in l2:\n        temp=[]\n        for j in i:\n            temp.append(j)\n        N.append(temp)\n    qn = str(\"Calculate difference between Matrix \"+ str(M) + \" and Matrix \" + str(N) +\" ?\")\n    qn = qn.replace(\"(\",\" ( \").replace(\")\",\" ) \").replace(\"[\",\" [ \").replace(\"]\",\" ] \").replace(\"*\",\" * \").replace(\"-\",\" - \")\n    k = re.sub('\\s+',' ',qn)\n#     print(qn)\n    questions.append(k)\n\n\n    M = Matrix(M)\n    N = Matrix(N)\n\n    ans = str(M-N)\n    ans = ans.replace(\"(\",\" ( \").replace(\")\",\" ) \").replace(\"[\",\" [ \").replace(\"]\",\" ] \").replace(\"*\",\" * \").replace(\"-\",\" - \")\n    k = re.sub('\\s+',' ',ans)\n#     print(ans)\n    answers.append(k)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Item = namedtuple('Item', 'question answer')\n\ndivision = 10\nsz = len(questions) \n\nstart = 0\nques = []\nans = []\nwhile start < sz:\n\tend = int(min(sz, start + sz // division))\n\tques.append(questions[start:end])\n\tans.append(answers[start:end])\n\tstart = end\n\ndf_2d = []\ndivision = len(ques)\nfor qno in range(len(ques)):\n\tq = ques[qno]\n\tan = ans[qno]\n\titems = []\n\tfor i in range(len(q)):\n\t\tquestion = q[i]\n\t\tanswer = an[i]\n\t\tif(len(question)>2):\n\t\t\t\titems.append(Item(question, answer))\n\n\tdf = pd.DataFrame.from_records(items, columns=['Question', 'Answer'])\n\tdf_2d.append(df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JRbjRUARf7xY",
        "colab_type": "code",
        "outputId": "e3647c46-65a7-47a3-b275-3f258df765b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "with open('./matrix_subtraction_questions.txt', 'w') as file:\n    for i in questions:\n        file.write(i)\nwith open('./matrix_subtraction_answers.txt', 'w') as file2:\n    for i in answers:\n        file2.write(i)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wNVhL5BxZZsh"
      },
      "cell_type": "markdown",
      "source": "### Creating the dataset of word problems\n\n*Please add the correct path to load the data file*"
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "52CngavAg1PM",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "input_exps_2d = []\nfor df in df_2d:\n  input_exps = list(df['Question'].values)\n  input_exps_2d.append(input_exps)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5BP-L86thNKh",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "target_exps_2d = []\nfor df in df_2d:\n  target_exps = list(df['Answer'].values)\n  target_exps_2d.append(target_exps)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GTRQklLBp-YC",
        "outputId": "9d349aa1-c279-4804-8234-cbf3defdcbc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Input: Word problem\ninput_exps_2d[0][:15]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "O9DL8tD-6iP9",
        "outputId": "1df2ac08-2fbb-4588-c561-2e2343f74e3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Target: Equation\ntarget_exps_2d[0][:15]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "v_MAlVH39Mlh",
        "outputId": "4d3c6082-4130-41a1-81bd-02ffbafa7f9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "len(pd.Series(input_exps)), len(pd.Series(input_exps).unique())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VL3-dcn8YMe3",
        "outputId": "4a281414-4f8d-4b7f-fdf5-50399263df45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "len(pd.Series(target_exps)), len(pd.Series(target_exps).unique())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A426ZXvuXSNd",
        "colab_type": "code",
        "outputId": "504b7063-68e4-4127-c3b7-0b18a9101818",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "questions[0:15]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mMz7YWDJXVz-",
        "colab_type": "code",
        "outputId": "3a549a2a-1b7f-4bd7-c536-0e545447cd93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "answers[0:15]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "p-0vtG7rdZdy"
      },
      "cell_type": "markdown",
      "source": "### Preprocessing and Tokenizing the Input and Target exps"
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8yx7HUtFYZri",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def preprocess_input(sentence):\n  '''\n  For the word problem, convert everything to lowercase, add spaces around all\n  punctuations and digits, and remove any extra spaces. \n  '''\n  sentence = sentence.lower().strip()\n  sentence = re.sub(r\"([?.!,’\\(\\),-])\", r\" \\1 \", sentence)\n  # sentence = re.sub(r\"add\",\"\",sentence)\n  # sentence = re.sub(r\",\",\"\",sentence)\n  sentence = re.sub(r'\\*\\*',r\" ^ \",sentence)\n  sentence = re.sub(r'\\*',r\" * \",sentence)\n  sentence = re.sub(r\"([0-9])\", r\" \\1 \", sentence)\n  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n  sentence = sentence.rstrip().strip()\n  return sentence",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IfyWFYbo-Fkt",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def preprocess_target(sentence):\n  '''\n  For the equation, convert it to lowercase and remove extra spaces\n  '''\n  sentence = sentence.lower().strip()\n  sentence = re.sub(r\"([\\(\\)?.!,’,-])\", r\" \\1 \", sentence)\n  sentence = re.sub(r\"([0-9])\", r\" \\1 \", sentence)\n  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n  sentence = re.sub(r'\\*\\*',r\" ^ \",sentence)\n  sentence = re.sub(r'\\*',r\" * \",sentence)\n  return sentence",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lEAS9242ZUT6",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "preprocessed_input_exps_2d = []\npreprocessed_target_exps_2d = []\nfor i in range(division):\n  input_exps = input_exps_2d[i]\n  target_exps = target_exps_2d[i]\n  preprocessed_input_exps = list(map(preprocess_input, input_exps))\n  preprocessed_target_exps = list(map(preprocess_target, target_exps))\n  preprocessed_input_exps_2d.append(preprocessed_input_exps)\n  preprocessed_target_exps_2d.append(preprocessed_target_exps)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aHAtgQkx6DoY",
        "colab_type": "code",
        "outputId": "a65f1a49-b0f0-4e83-ed63-2b2da305d7d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(len(preprocessed_target_exps))\nprint(len(preprocessed_input_exps_2d))\nprint(len(preprocessed_target_exps_2d))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sy-bGm81a8yl",
        "outputId": "798e0cac-2ebe-4177-a664-91319f92ac40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "preprocessed_input_exps_2d[9][:5]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "g15oK28mbA89",
        "outputId": "49ab551d-0bbb-4308-c0b6-7c0b32d52341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "preprocessed_target_exps_2d[9][:5]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aKhkEBsfbJaN",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def tokenize(lang):\n  '''\n  Tokenize the given list of strings and return the tokenized output\n  along with the fitted tokenizer.\n  '''\n  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n  lang_tokenizer.fit_on_texts(lang)\n  tensor = lang_tokenizer.texts_to_sequences(lang)\n  return tensor, lang_tokenizer",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GL_eLZZAbsHJ",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "input_tensor_2d = []\ninp_lang_tokenizer_2d = []\nfor i in range(division):\n  input_tensor, inp_lang_tokenizer = tokenize(preprocessed_input_exps)\n  input_tensor_2d.append(input_tensor)\n  inp_lang_tokenizer_2d.append(inp_lang_tokenizer)\n# print(input_tensor)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0DpVACvi86vX",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "# print(len(preprocessed_input_exps))\n# print(inp_lang_tokenizer_2d[9].word_index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fI2k73bDceJ_",
        "outputId": "5c41486c-5d55-4525-8042-5d39f33878dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(len(input_tensor_2d))\nlen(inp_lang_tokenizer_2d[9].word_index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sVIuwYu9b7fH",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "target_tensor_2d = []\ntarg_lang_tokenizer_2d = []\nfor i in range(division):\n  target_tensor, targ_lang_tokenizer = tokenize(preprocessed_target_exps)\n  target_tensor_2d.append(target_tensor)\n  targ_lang_tokenizer_2d.append(targ_lang_tokenizer)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WqLOtR5h5s9b",
        "outputId": "24e0b894-1b04-49c0-a4d3-750ab5c339c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "old_len = len(targ_lang_tokenizer_2d[9].word_index)\nprint(old_len)\nprint(targ_lang_tokenizer_2d[9].word_index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "z0drKV5Vx6k2",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def append_start_end(x,last_int):\n  '''\n  Add integers for start and end tokens for input/target exps\n  '''\n  l = []\n  l.append(last_int+1)\n  l.extend(x)\n  l.append(last_int+2)\n  return l",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HepsumSnyLL5",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "input_tensor_list_2d = []\ntarget_tensor_list_2d = []\nfor j in range(division):\n  input_tensor_list = [append_start_end(i,len(inp_lang_tokenizer.word_index)) for i in input_tensor_2d[j]]\n  target_tensor_list = [append_start_end(i,len(targ_lang_tokenizer.word_index)) for i in target_tensor_2d[j]]\n  input_tensor_list_2d.append(input_tensor_list)\n  target_tensor_list_2d.append(target_tensor_list)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "onOq9Cj4CajL",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Pad all sequences such that they are of equal length\ninput_tensor_2d = []\ntarget_tensor_2d = []\nfor i in range(division):\n  input_tensor_list = input_tensor_list_2d[i]\n  target_tensor_list = target_tensor_list_2d[i]\n  input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor_list, padding='post')\n  target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor_list, padding='post')\n  input_tensor_2d.append(input_tensor)\n  target_tensor_2d.append(target_tensor)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Da9_X2TF0qe-",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "# input_tensor_2d[51].shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vimEgxw8tCjz",
        "outputId": "9c511609-fc07-4415-95ce-388d5b8712cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "target_tensor_2d[0].shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8bpeONmscRzr",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Here we are increasing the vocabulary size of the target, by adding a\n# few extra vocabulary words (which will not actually be used) as otherwise the\n# # small vocab size causes issues downstream in the network.\nkeys = [str(i) for i in range(10,51)]\nfor i,k in enumerate(keys):\n  for j in range(division):\n    targ_lang_tokenizer_2d[j].word_index[k]=len(targ_lang_tokenizer.word_index)+i+4",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kbO9GO0sb4-3",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "# len(targ_lang_tokenizer_2d[23].word_index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lWKtDz1pdV11"
      },
      "cell_type": "markdown",
      "source": "### Create a tf.data dataset"
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LWn3alPwcIMw",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_data = []\ntest_targ = []\ninput_tensor_train_2d = []\ntarget_tensor_train_2d = []\nfor i in range(division):\n  input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor_2d[i],\n                                                                                                target_tensor_2d[i],\n                                                                                                test_size=0.05,\n                                                                                                random_state=42)\n  input_tensor_train_2d.append(input_tensor_train)\n  target_tensor_train_2d.append(target_tensor_train)\n  for j in input_tensor_val:\n    test_data.append(j)\n  for j in target_tensor_val:\n    test_targ.append(j)\ninput_tensor_val = test_data\ntarget_tensor_val = test_targ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JkvMMb7NUG1l",
        "outputId": "341db11c-e834-44fb-864b-79c96444410e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(len(test_data))\nprint(len(test_targ))\nprint(len(input_tensor_train_2d))\nprint(len(target_tensor_train_2d))\nprint(len(input_tensor_train))\nlen(input_tensor_train_2d[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "X8VnUqe9UE9P",
        "outputId": "2a6f0f7c-c98a-493a-ea79-1a013de9ed18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "len(input_tensor_val)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BFxzn930dDNs",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "BUFFER_SIZE = len(input_tensor_train_2d[0])\nBATCH_SIZE = 128\nsteps_per_epoch = len(input_tensor_train_2d[0])//BATCH_SIZE\ndataset_2d = []\nfor i in range(division):\n  dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train_2d[i], target_tensor_train_2d[i])).shuffle(BUFFER_SIZE)\n  dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n  dataset_2d.append(dataset)\nnum_layers = 4\nd_model = 128\ndff = 512\nnum_heads = 8\n# input_vocab_size = input_tensor_2d[0].shape[1]+40\ninput_vocab_size = len(inp_lang_tokenizer_2d[0].word_index)+60\ntarget_vocab_size = len(targ_lang_tokenizer_2d[0].word_index)+60\ndropout_rate = 0.0",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CSKWb6kjCz_b",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "# print(input_vocab_size, target_vocab_size)\n# for i in range(len(targ_lang_tokenizer_2d)):\n#   print(len(targ_lang_tokenizer_2d[i].word_index))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6BnLEanaeNOX",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "# example_input_batch, example_target_batch = next(iter(dataset))\n# example_input_batch.shape, example_target_batch.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Mn4vAkn4vFg",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "# print(input_vocab_size)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lLihV1nlcqH8"
      },
      "cell_type": "markdown",
      "source": "### Transformer Model"
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "nRxTAfRkdRh5"
      },
      "cell_type": "markdown",
      "source": "#### Positional Encoding"
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PqjLcutlcubG",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "# We provide positional information about the data to the model,\n# otherwise each sentence will be treated as Bag of Words\ndef get_angles(pos, i, d_model):\n  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n  return pos * angle_rates",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "B4LBjCfbd36x",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def positional_encoding(position, d_model):\n  print(position,d_model)\n  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                          np.arange(d_model)[np.newaxis, :],\n                          d_model)\n  \n  # apply sin to even indices in the array; 2i\n  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n  \n  # apply cos to odd indices in the array; 2i+1\n  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    \n  pos_encoding = angle_rads[np.newaxis, ...]\n    \n  return tf.cast(pos_encoding, dtype=tf.float32)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "i0ARP8oYeGpb"
      },
      "cell_type": "markdown",
      "source": "#### Masking"
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XPU51VjMeIOp",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "# mask all elements are that not words (padding) so that it is not treated as input\ndef create_padding_mask(seq):\n  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n  \n  # add extra dimensions to add the padding\n  # to the attention logits.\n  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NLLyLy3_eS69",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def create_look_ahead_mask(size):\n  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n  return mask",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NsFCUWbNz-jo",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "for i in range(division):\n  dataset_2d[i] = dataset_2d[i].prefetch(tf.data.experimental.AUTOTUNE)\n  # print(dataset_2d[i])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fVbSOYuyeaPm"
      },
      "cell_type": "markdown",
      "source": "\n#### Attention"
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jW-49jyBeZy0",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def scaled_dot_product_attention(q, k, v, mask):\n  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n  \n  # scale matmul_qk\n  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n  # add the mask to the scaled tensor.\n  if mask is not None:\n    scaled_attention_logits += (mask * -1e9)  \n\n  # softmax is normalized on the last axis (seq_len_k) so that the scores\n  # add up to 1.\n  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n  return output, attention_weights",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VhCANAddefdq",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "class MultiHeadAttention(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads):\n    super(MultiHeadAttention, self).__init__()\n    self.num_heads = num_heads\n    self.d_model = d_model\n    \n    assert d_model % self.num_heads == 0\n    \n    self.depth = d_model // self.num_heads\n    \n    self.wq = tf.keras.layers.Dense(d_model)\n    self.wk = tf.keras.layers.Dense(d_model)\n    self.wv = tf.keras.layers.Dense(d_model)\n    \n    self.dense = tf.keras.layers.Dense(d_model)\n        \n  def split_heads(self, x, batch_size):\n    \"\"\"Split the last dimension into (num_heads, depth).\n    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n    \"\"\"\n    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n    return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n  def call(self, v, k, q, mask):\n    batch_size = tf.shape(q)[0]\n    \n    q = self.wq(q)  # (batch_size, seq_len, d_model)\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\n    \n    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n    \n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    scaled_attention, attention_weights = scaled_dot_product_attention(\n        q, k, v, mask)\n    \n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n    concat_attention = tf.reshape(scaled_attention, \n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n        \n    return output, attention_weights",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gq-Kw2rLgpme"
      },
      "cell_type": "markdown",
      "source": "#### Pointwise Feed forward network"
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9mNE0fJvefaO",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def point_wise_feed_forward_network(d_model, dff):\n  return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n  ])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "EgfEMiove1Zn"
      },
      "cell_type": "markdown",
      "source": "#### Encoder Layer"
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_qQc6o64eySC",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "class EncoderLayer(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads, dff, rate=0.1):\n    super(EncoderLayer, self).__init__()\n\n    self.mha = MultiHeadAttention(d_model, num_heads)\n    self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n    # normalize data per feature instead of batch\n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    \n    self.dropout1 = tf.keras.layers.Dropout(rate)\n    self.dropout2 = tf.keras.layers.Dropout(rate)\n    \n  def call(self, x, training, mask):\n    # Multi-head attention layer\n    attn_output, _ = self.mha(x, x, x, mask) \n    attn_output = self.dropout1(attn_output, training=training)\n    # add residual connection to avoid vanishing gradient problem\n    out1 = self.layernorm1(x + attn_output)\n    \n    # Feedforward layer\n    ffn_output = self.ffn(out1)\n    ffn_output = self.dropout2(ffn_output, training=training)\n    # add residual connection to avoid vanishing gradient problem\n    out2 = self.layernorm2(out1 + ffn_output)\n    return out2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eqLdtAAPorvt"
      },
      "cell_type": "markdown",
      "source": "#### Encoder"
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2WsCLoYEfC1L",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Encoder(tf.keras.layers.Layer):\n  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n               maximum_position_encoding, rate=0.1):\n    super(Encoder, self).__init__()\n\n    self.d_model = d_model\n    self.num_layers = num_layers\n    \n    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n    self.pos_encoding = positional_encoding(maximum_position_encoding, \n                                            self.d_model)\n    \n    # Create encoder layers (count: num_layers)\n    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n                       for _ in range(num_layers)]\n  \n    self.dropout = tf.keras.layers.Dropout(rate)\n        \n  def call(self, x, training, mask):\n    seq_len = tf.shape(x)[1]\n    # adding embedding and position encoding.\n    x = self.embedding(x) \n    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n    x +=  self.pos_encoding[:, :seq_len, :]#problem here\n\n    x = self.dropout(x, training=training)\n    \n    for i in range(self.num_layers):\n      x = self.enc_layers[i](x, training, mask)\n    \n    return x ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "R6bSvwJ1e_g8"
      },
      "cell_type": "markdown",
      "source": "#### Decoder Layer"
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YQgt7OcdeyKr",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "class DecoderLayer(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads, dff, rate=0.1):\n    super(DecoderLayer, self).__init__()\n\n    self.mha1 = MultiHeadAttention(d_model, num_heads)\n    self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n    self.ffn = point_wise_feed_forward_network(d_model, dff)\n \n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    \n    self.dropout1 = tf.keras.layers.Dropout(rate)\n    self.dropout2 = tf.keras.layers.Dropout(rate)\n    self.dropout3 = tf.keras.layers.Dropout(rate)\n    \n    \n  def call(self, x, enc_output, training, \n           look_ahead_mask, padding_mask):\n\n    # Masked multihead attention layer (padding + look-ahead)\n    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n    attn1 = self.dropout1(attn1, training=training)\n    # again add residual connection\n    out1 = self.layernorm1(attn1 + x)\n    \n    # Masked multihead attention layer (only padding)\n    # with input from encoder as Key and Value, and input from previous layer as Query\n    attn2, attn_weights_block2 = self.mha2(\n        enc_output, enc_output, out1, padding_mask)\n    attn2 = self.dropout2(attn2, training=training)\n    # again add residual connection\n    out2 = self.layernorm2(attn2 + out1)\n    \n    # Feedforward layer\n    ffn_output = self.ffn(out2)\n    ffn_output = self.dropout3(ffn_output, training=training)\n    # again add residual connection\n    out3 = self.layernorm3(ffn_output + out2)\n    return out3, attn_weights_block1, attn_weights_block2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lxvJ4M-pouYH"
      },
      "cell_type": "markdown",
      "source": "#### Decoder"
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "W8M_UsFHfCtH",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Decoder(tf.keras.layers.Layer):\n  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n               maximum_position_encoding, rate=0.1):\n    super(Decoder, self).__init__()\n\n    self.d_model = d_model\n    self.num_layers = num_layers\n     \n    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n    \n    # Create decoder layers (count: num_layers)\n    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n                       for _ in range(num_layers)]\n    self.dropout = tf.keras.layers.Dropout(rate)\n    \n  def call(self, x, enc_output, training, \n           look_ahead_mask, padding_mask):\n\n    seq_len = tf.shape(x)[1]\n    attention_weights = {}\n    \n    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n    \n    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n    \n    x += self.pos_encoding[:,:seq_len,:]\n    \n    x = self.dropout(x, training=training)\n\n    for i in range(self.num_layers):\n      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n                                             look_ahead_mask, padding_mask)\n      \n      # store attenion weights, they can be used to visualize while translating\n      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n    \n    return x, attention_weights",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_wlWv0HSptaN"
      },
      "cell_type": "markdown",
      "source": "#### Transformer"
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "t3ldNVklfaMY",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Transformer(tf.keras.Model):\n  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n               target_vocab_size, pe_input, pe_target, rate=0.1):\n    super(Transformer, self).__init__()\n\n    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n                           input_vocab_size, pe_input, rate)\n\n    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n                           target_vocab_size, pe_target, rate)\n\n    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n    \n  def call(self, inp, tar, training, enc_padding_mask, \n           look_ahead_mask, dec_padding_mask):\n\n    # Pass the input to the encoder\n    enc_output = self.encoder(inp, training, enc_padding_mask) #problem here\n    \n    # Pass the encoder output to the decoder\n    dec_output, attention_weights = self.decoder(\n        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n    \n    # Pass the decoder output to the last linear layer\n    final_output = self.final_layer(dec_output)\n    \n    return final_output, attention_weights",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QLT6USM-htw3"
      },
      "cell_type": "markdown",
      "source": "#### Optimizer and Loss"
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "588zG6wOfaGd",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n  def __init__(self, d_model, warmup_steps=4000):\n    super(CustomSchedule, self).__init__()\n    \n    self.d_model = d_model\n    self.d_model = tf.cast(self.d_model, tf.float32)\n\n    self.warmup_steps = warmup_steps\n    \n  def __call__(self, step):\n    arg1 = tf.math.rsqrt(step)\n    arg2 = step * (self.warmup_steps ** -1.5)\n    \n    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZfEAPe3FfZ-8",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "learning_rate = CustomSchedule(d_model)\n\n# Adam optimizer with a custom learning rate\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n                                     epsilon=1e-9)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Ree_sJLFfZfm",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "il10DQXPefP-",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def loss_function(real, pred):\n  # Apply a mask to paddings (0)\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n  \n  return tf.reduce_mean(loss_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EzVTb8RmefDd",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n    name='train_accuracy')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "e6QXJV9rh8nc",
        "outputId": "c0cf2391-11cc-4c70-da32-5119166961e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "transformer = Transformer(num_layers, d_model, num_heads, dff,\n                          input_vocab_size, target_vocab_size, \n                          pe_input=input_vocab_size, \n                          pe_target=target_vocab_size,\n                          rate=dropout_rate)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iK5IKzTK2jDI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mnNv_E9Wh8jl",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def create_masks(inp, tar):\n  # Encoder padding mask\n  enc_padding_mask = create_padding_mask(inp)\n  \n  # Decoder padding mask\n  dec_padding_mask = create_padding_mask(inp)\n  \n  # Look ahead mask (for hiding the rest of the sequence in the 1st decoder attention layer)\n  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n  dec_target_padding_mask = create_padding_mask(tar)\n  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n  \n  return enc_padding_mask, combined_mask, dec_padding_mask",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Jj0D4HuIatpn"
      },
      "cell_type": "markdown",
      "source": "#### Training"
    },
    {
      "metadata": {
        "id": "gQ6aI1Cfgief",
        "colab_type": "code",
        "outputId": "fa768a2f-0559-4149-d54f-3bd7b1d6e5a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "preprocessed_input_exps_2d[0][:10]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WJpFwLF8gNJj",
        "colab_type": "code",
        "outputId": "6895044d-f943-4573-b8d9-ceaf4e2f3ea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "for i in input_tensor_2d[0][:10]:\n  print(i)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OI-q54bLgXDi",
        "colab_type": "code",
        "outputId": "07c53203-20e0-4f8f-eedd-3914b3789c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(inp_lang_tokenizer_2d[0].word_index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3lnynkFNgj1x",
        "colab_type": "code",
        "outputId": "078a36a1-c1ce-436a-de92-115c81ad78ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "preprocessed_target_exps_2d[0][:10]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VQH9od-ZgSLl",
        "colab_type": "code",
        "outputId": "2a43f3b2-0206-4436-a902-05167697ff0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "for i in target_tensor_2d[0][:10]:\n  print(i)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m2N1QHDngckv",
        "colab_type": "code",
        "outputId": "89ae26b7-d40a-4c7d-f536-7f627091bb7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(targ_lang_tokenizer_2d[0].word_index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aOcltIu0h8cr",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "EPOCHS = 100\nepoch_num = 0\ndef train_step(inp, tar):\n  tar_inp = tar[:, :-1]\n  tar_real = tar[:, 1:]\n  \n  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n  # print('This is', dec_padding_mask)\n  \n  with tf.GradientTape() as tape:\n    predictions, _ = transformer(inp, tar_inp, \n                                 True, \n                                 enc_padding_mask, \n                                 combined_mask, \n                                 dec_padding_mask) #problem here\n    loss = loss_function(tar_real, predictions)\n\n  gradients = tape.gradient(loss, transformer.trainable_variables)    \n  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n  \n  train_loss(loss)\n  train_accuracy(tar_real, predictions)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lEqQmYLZh8R3",
        "outputId": "c9e7ff14-c5ef-46df-8889-878a9d22497c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "for epoch in range(epoch_num, EPOCHS):\n  start = time.time()\n  \n  train_loss.reset_states()\n  train_accuracy.reset_states()\n  \n  # inp -> question, tar -> equation\n  for (batch, (inp, tar)) in enumerate(dataset_2d[epoch%division]):\n    train_step(inp, tar) #problem here\n    \n    if batch % 100 == 0:\n      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n      \n  # ckpt_save_path = ckpt_manager.save()\n  # print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n  #                                                        ckpt_save_path))\n    \n  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n                                                train_loss.result(), \n                                                train_accuracy.result()))\n\n  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fpi8JaiblDOh"
      },
      "cell_type": "markdown",
      "source": "### Evaluate"
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Op5A3ikpil_l",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def evaluate(inp_sentence):\n  start_token = [len(inp_lang_tokenizer.word_index)+1]\n  end_token = [len(inp_lang_tokenizer.word_index)+2]\n  \n  # inp sentence is the word problem, hence adding the start and end token\n  inp_sentence = start_token + [inp_lang_tokenizer.word_index[i] for i in preprocess_input(inp_sentence).split(' ')]+end_token\n  encoder_input = tf.expand_dims(inp_sentence, 0)\n  \n  # start with equation's start token\n  decoder_input = [old_len+1]\n  output = tf.expand_dims(decoder_input, 0)\n    \n  for i in range(MAX_LENGTH):\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n        encoder_input, output)\n  \n    predictions, attention_weights = transformer(encoder_input, \n                                                 output,\n                                                 False,\n                                                 enc_padding_mask,\n                                                 combined_mask,\n                                                 dec_padding_mask)\n    \n    # select the last word from the seq_len dimension\n    predictions = predictions[: ,-1:, :] \n    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n    \n    # return the result if the predicted_id is equal to the end token\n    if predicted_id == old_len+2:\n      return tf.squeeze(output, axis=0), attention_weights\n    \n    # concatentate the predicted_id to the output which is given to the decoder\n    # as its input.\n    output = tf.concat([output, predicted_id], axis=-1)\n  return tf.squeeze(output, axis=0), attention_weights",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "myLyxXYpiltz",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def plot_attention_weights(attention, sentence, result, layer):\n  fig = plt.figure(figsize=(16, 8))\n  \n  sentence = preprocess_input(sentence)\n  \n  attention = tf.squeeze(attention[layer], axis=0)\n  \n  for head in range(attention.shape[0]):\n    ax = fig.add_subplot(2, 4, head+1)\n    \n    # plot the attention weights\n    ax.matshow(attention[head][:-1, :], cmap='viridis')\n    \n    fontdict = {'fontsize': 10}\n    \n    ax.set_xticks(range(len(sentence.split(' '))+2))\n    ax.set_yticks(range(len([targ_lang_tokenizer.index_word[i] for i in list(result.numpy()) \n                        if i < len(targ_lang_tokenizer.word_index) and i not in [0,old_len+1,old_len+2]])+3))\n    \n    \n    ax.set_ylim(len([targ_lang_tokenizer.index_word[i] for i in list(result.numpy()) \n                        if i < len(targ_lang_tokenizer.word_index) and i not in [0,old_len+1,old_len+2]]), -0.5)\n        \n    ax.set_xticklabels(\n        ['<start>']+sentence.split(' ')+['<end>'], \n        fontdict=fontdict, rotation=90)\n    \n    ax.set_yticklabels([targ_lang_tokenizer.index_word[i] for i in list(result.numpy()) \n                        if i < len(targ_lang_tokenizer.word_index) and i not in [0,old_len+1,old_len+2]], \n                       fontdict=fontdict)\n    \n    ax.set_xlabel('Head {}'.format(head+1))\n  \n  plt.tight_layout()\n  plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tiVDtj2FoNZr",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "MAX_LENGTH = 100",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "joZ3POuph8NH",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def translate(sentence, plot=''):\n  result, attention_weights = evaluate(sentence)\n  # print('result',list(result.numpy()))\n\n  # use the result tokens to convert prediction into a list of characters\n  # (not inclusing padding, start and end tokens)\n  predicted_sentence = [targ_lang_tokenizer.index_word[i] for i in list(result.numpy()) if (i < len(targ_lang_tokenizer.word_index) and i not in [0, old_len + 1,old_len + 2])]  \n\n  print('Input: {}'.format(sentence))\n  print('Predicted translation: {}'.format(' '.join(predicted_sentence)))\n  \n  if plot:\n    plot_attention_weights(attention_weights, sentence, result, plot)\n  return predicted_sentence",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "spiU0QDI2egM"
      },
      "cell_type": "markdown",
      "source": "### Get Accuracy and Corpus BLEU"
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tIQBWfa03ujc",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def evaluate_results(inp_sentence):\n  start_token = [len(inp_lang_tokenizer.word_index)+1]\n  end_token = [len(inp_lang_tokenizer.word_index)+2]\n  \n  # inp sentence is the word problem, hence adding the start and end token\n  inp_sentence = start_token + list(inp_sentence.numpy()[0]) + end_token\n  \n  encoder_input = tf.expand_dims(inp_sentence, 0)\n  \n  \n  decoder_input = [old_len+1]\n  output = tf.expand_dims(decoder_input, 0)\n    \n  for i in range(MAX_LENGTH):\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n        encoder_input, output)\n  \n    # predictions.shape == (batch_size, seq_len, vocab_size)\n    predictions, attention_weights = transformer(encoder_input, \n                                                 output,\n                                                 False,\n                                                 enc_padding_mask,\n                                                 combined_mask,\n                                                 dec_padding_mask)\n    \n    # select the last word from the seq_len dimension\n    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n\n    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n    \n    # return the result if the predicted_id is equal to the end token\n    if predicted_id == old_len+2:\n      return tf.squeeze(output, axis=0), attention_weights\n    \n    # concatentate the predicted_id to the output which is given to the decoder\n    # as its input.\n    output = tf.concat([output, predicted_id], axis=-1)\n\n  return tf.squeeze(output, axis=0), attention_weights",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rTs5Q4Qf2qNv",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "dataset_val = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).shuffle(BUFFER_SIZE)\ndataset_val = dataset_val.batch(1, drop_remainder=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HVyDC8zyo0uM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "905e62d4-7643-432f-df0c-3ce7b46e1807",
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_true = []\ny_pred = []\nacc_cnt = 0\nquestions = []\nfor i in preprocessed_input_exps_2d:\n  for j in i:\n    questions.append(j)\nanswers = []\nfor i in preprocessed_target_exps_2d:\n  for j in i:\n    answers.append(j)\n\n\ntot = 200\nprint(tot)\nfor j in range(tot):\n  # prob = \"\"\n  check_str = ' '.join([inp_lang_tokenizer.index_word[i] for i in input_tensor_val[j] if i not in [0, len(inp_lang_tokenizer.word_index)+1, len(inp_lang_tokenizer.word_index)+2]])\n  ans = translate(check_str, plot='')\n  print(check_str)\n  ind = questions.index(check_str)\n  act_ans = answers[ind].replace(\" \",\"\")\n  pred_ans = \"\"\n  for i in range(0,len(ans)):\n    pred_ans += ans[i]\n  print(\"Example number = \", j)\n  print(\"predicted answer = \",pred_ans)\n  print(\"actual answer = \",act_ans)\n  if(pred_ans==act_ans):\n    acc_cnt += 1\n  else:\n    print(\"Wrong answer\")\nprint(acc_cnt, 'out of', tot, 'correctly calculated')\nprint('Accuracy: ', acc_cnt * 100 / tot)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(acc_cnt, 'out of', tot, 'correctly calculated')\nprint('Accuracy: ', acc_cnt * 100 / tot)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "0SFJifWj7AtQ"
      },
      "cell_type": "markdown",
      "source": "#### Translation"
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GEmx2HVO6_cC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e67f524-dfaa-414f-e580-ebf168a0c22a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# print(input_tensor_val)\ncheck_str = ' '.join([inp_lang_tokenizer.index_word[i] for i in input_tensor_val[10] if i not in [0,\n                                                                                                  len(inp_lang_tokenizer.word_index)+1,\n                                                                                                  len(inp_lang_tokenizer.word_index)+2]])\nprint(check_str)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "A_fyWhFZ6_cG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eca90312-b297-48ac-a132-5111722e274c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "check_str",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-fPqedk46_cM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "e4deef09-ca05-42c8-c0fb-37972022c459",
        "trusted": true
      },
      "cell_type": "code",
      "source": "translate(check_str,\n          plot='decoder_layer4_block2')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EUVJbPnJ6_cV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ea08bef-2482-4dbb-e460-a4926f456e21",
        "trusted": true
      },
      "cell_type": "code",
      "source": "check_str = ' '.join([inp_lang_tokenizer.index_word[i] for i in input_tensor_val[422] if i not in [0,\n                                                                                                  len(inp_lang_tokenizer.word_index)+1,\n                                                                                                  len(inp_lang_tokenizer.word_index)+2]])\ncheck_str",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "RFbRPr916_cY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "51ed09b1-8ebe-41b6-e9bd-f218fd35a685",
        "trusted": true
      },
      "cell_type": "code",
      "source": "translate(check_str,\n          plot='decoder_layer4_block2')",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sequence_next_term_1.5mil.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}